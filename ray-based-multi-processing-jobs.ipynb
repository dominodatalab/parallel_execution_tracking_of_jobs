{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83475e7a-da74-49dc-838d-d3be3a187968",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Add your model JAR file to the following location\n",
    "```\n",
    "domino_project_name=os.environ['DOMINO_PROJECT_NAME']\n",
    "root_folder = f'/domino/datasets/local/{domino_project_name}/jar/model.jar'\n",
    "```\n",
    "\n",
    "The input files will be placed in the folder \n",
    "```\n",
    "/mnt/input/\n",
    "          /high_r0.yaml\n",
    "          /large_experiment_1.yaml\n",
    "          /low_r0.yaml\n",
    "```\n",
    "\n",
    "When the worker process is called `train_function` the yaml is sent to it as a parameter (`dict`) along with the path to the executable jar.\n",
    "The worker writes the output locally to its `/tmp/` folder and writes the output as artifacts to the experiment run. Except for the executable jar\n",
    "no other data location is shared between the workspace and the ray workers. Method to distribute the JAR to ray workers also exist. We chose this for simplicity\n",
    "\n",
    "\n",
    "## Best Practices for Ray Worker Configuration\n",
    "\n",
    "Say you want to process a 1000 files (1000 trials). Starting 1000 worker Ray cluster on a small HW Tier is not the best strategy for the following reason - You might need a 1000 nodes. K8s clusters are complex to manage operationally for such a large number of nodes. Typically 100-200 is a reasonable number. More than that, needs finer tuning of your K8s cluster for stability\n",
    "\n",
    "\n",
    "Instead use a larger HW Tier say 64 cores. And choose 16 workers each using up all of the HW Tier (Ask our PS team how). Inside each worker process 64 files in 64 threads (processes) and return 64 results from each worker. You will achieve a high degree of parallelism (1024) with just 16 nodes added to your Ray cluster.\n",
    "\n",
    "If you want to use GPU a similar strategy applies. NVIDIA now supports the MIG architecture which allows for GPU sharing. You could divide a single GPU into 56 logical GPU's. This allows for parallelism of upto 56. Ask our PS Team for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237abd6-c21c-48c2-8c77-a96aa981f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import uuid\n",
    "import pandas\n",
    "import yaml\n",
    "import json\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID\n",
    "import ray\n",
    "import yaml\n",
    "\n",
    "domino_project_name=os.environ['DOMINO_PROJECT_NAME']\n",
    "root_folder = f'/domino/datasets/local/{domino_project_name}/'\n",
    "\n",
    "\n",
    "def write_yaml_file(base_path,f_name,obj):\n",
    "    input_file = f'{base_path}/{f_name}'\n",
    "    with open(f'{base_path}/{f_name}', 'w') as f:        \n",
    "        yaml.dump(obj, f, allow_unicode=True, default_flow_style=False)\n",
    "    return input_file\n",
    "\n",
    "from pathlib import Path\n",
    "@ray.remote\n",
    "def train_function(iteration_no,exp_id,parent_run_id,input_yaml,executable):    \n",
    "    root, extension  = os.path.splitext(os.path.basename(input_yaml[0]))\n",
    "    with mlflow.start_run(experiment_id=exp_id,run_name=f\"iteration{iteration_no}-{root}\",tags={\n",
    "            MLFLOW_PARENT_RUN_ID: parent_run_id\n",
    "        },nested=True) as run:\n",
    "        input_folder = f'/tmp/input/'\n",
    "        \n",
    "        Path(input_folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        yaml_f_name = input_yaml[0]\n",
    "        yaml_o = input_yaml[1]\n",
    "        input_file = write_yaml_file(input_folder,yaml_f_name,yaml_o)\n",
    "      \n",
    "        run_params = yaml_o['baseScenario']        \n",
    "        \n",
    "        mlflow.log_params(run_params)\n",
    "        \n",
    "        o = str(uuid.uuid4())\n",
    "        output_folder = f'/tmp/output/{o}'\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "        out = subprocess.run([\"java\", \"-jar\", executable, \"-o\", output_folder, \"-t=1\", input_file ]) \n",
    "        \n",
    "        if out.returncode==0:        \n",
    "            experiment_report=pandas.read_csv(f'{output_folder}/experiment_report.tsv',sep='\\t').to_json()                        \n",
    "            person_property_report=pandas.read_csv(f'{output_folder}/person_property_report.tsv',sep='\\t').to_json()                        \n",
    "            \n",
    "            mlflow.log_artifact(f'{output_folder}/experiment_report.tsv',\"results\")                        \n",
    "            mlflow.log_artifact(f'{output_folder}/person_property_report.tsv',\"results\")\n",
    "            \n",
    "            data=pandas.read_csv(f'{output_folder}/person_property_report.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "            results = {}\n",
    "            days=[]\n",
    "            for index, row in data.iterrows():\n",
    "                scenario = row['scenario']\n",
    "                day = row['day']\n",
    "                region = row['region']\n",
    "                infection_status = row['value']\n",
    "                person_count = row['person_count']\n",
    "                if day not in results:\n",
    "                    results[day]={}\n",
    "                    days.append(day)\n",
    "                results[day][f\"Scenario-{scenario}-Infection_Status-{infection_status}\"]=person_count\n",
    "            for day in days:\n",
    "                 mlflow.log_metrics(results[day],step=day)\n",
    "        print('Done logging metrics')\n",
    "        return {\"success\": (out.returncode==0), \"i\": input_file, \"e\":experiment_report, \"p\":person_property_report}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f40690-ff0d-4e43-ad6a-4deaa5040468",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a MLFLOW Experiment\n",
    "\n",
    "You can call it anything you like. I simply created a name using the following :\n",
    "\n",
    "```\n",
    "        user_name = os.environ['DOMINO_USER_NAME']\n",
    "        exp_name = f'DEMO-{user_name}'\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b334c-557e-4f6e-99dc-a7c0754a48ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "user_name = os.environ['DOMINO_USER_NAME']\n",
    "project_id = os.environ['DOMINO_PROJECT_ID'] \n",
    "project_name = os.environ['DOMINO_PROJECT_NAME'] \n",
    "exp_name = f'demov2-{project_name}-{user_name}'\n",
    "exp = mlflow.get_experiment_by_name(exp_name)    \n",
    "if not exp:\n",
    "    print('Experiment Not Found Create it')\n",
    "    mlflow.create_experiment(exp_name)    \n",
    "    exp = mlflow.get_experiment_by_name(exp_name) \n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2f67-4892-4927-ace8-98f4f7a6c277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Initialize the Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8162e0d-eb24-4c7e-b52c-36e98773f601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ray\n",
    "import mlflow\n",
    "import boto3\n",
    "if not ray.is_initialized():\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    address=f\"ray://{service_host}:{service_port}\"\n",
    "    temp_dir='/mnt/data//{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "    ray.init(address=address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb496e5f-4ffe-49fc-8af1-637737b9962d",
   "metadata": {},
   "source": [
    "### Add your input files to the folder `/mnt/input` and create a list of these file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa96594-dbac-42a5-ad1d-a454e607399d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def get_input_files():\n",
    "    input_folder = f\"/mnt/input\"\n",
    "    input_files=[]\n",
    "    for file in listdir(input_folder):\n",
    "        if isfile(join(input_folder, file)):\n",
    "            with open(f'{input_folder}/{file}', 'r') as i:\n",
    "                data = yaml.safe_load(i)\n",
    "                input_files.append((file,data))\n",
    "\n",
    "    return input_files\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e09b9f-2a4f-4c25-86ad-2d607ba3b626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "d = get_input_files()\n",
    "def update_input_parameters(input_files):\n",
    "    new_files = []\n",
    "    for i in input_files:    \n",
    "        new_i = []\n",
    "        \n",
    "        x = i[1]            \n",
    "        l = x['experimentDimensions']\n",
    "        ll = []\n",
    "        for l in x['experimentDimensions']:\n",
    "            if (l['name']=='r0'):\n",
    "                updated_val = l['levels'][0]['parameters']['r0'] + 0.1      \n",
    "                l['levels'][0]['parameters']['r0'] = updated_val\n",
    "                l['levels'][0]['label'] = str(updated_val)\n",
    "            ll.append(l)\n",
    "        x['experimentDimensions'] = ll    \n",
    "        \n",
    "        new_i.append(x)\n",
    "        new_files.append((i[0],x))\n",
    "    return new_files\n",
    "#d = update_input_parameters(d)\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4bd575-8656-4c74-a3a1-7b17c1faabae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Start Experiment\n",
    "import mlflow\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now() # current date and time\n",
    "now_str = now.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "#End any previous run if any\n",
    "mlflow.end_run()\n",
    "\n",
    "data_set_folder='/domino/datasets/local/{}'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "model_executable=f'{data_set_folder}/jar/model.jar'\n",
    "\n",
    "input_files = get_input_files()\n",
    "\n",
    "with mlflow.start_run(experiment_id=exp.experiment_id,run_name=f\"root-{now_str}\") as parent_run:\n",
    "    parent_run_id=parent_run.info.run_id    \n",
    "    mlflow.log_param(f\"model_executable\", model_executable)\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    iteration_max=2\n",
    "    iteration_no=0\n",
    "    while iteration_no < iteration_max:        \n",
    "        print(f'\\n\\nStarting iteration {iteration_no}')\n",
    "        for index, f in enumerate(input_files, start=0):       \n",
    "            mlflow.log_param(f\"iteration_{iteration_no}_input_{index}\", f[0])\n",
    "        results = ray.get([train_function.remote(iteration_no,exp.experiment_id,parent_run_id,input_yaml,model_executable) for input_yaml in input_files])\n",
    "        for r in results:            \n",
    "            if r[\"success\"]:\n",
    "                print(f'Trial successful for input {r[\"i\"]}')\n",
    "                print(\"You can evaluate here, change input files and rerun. We always assume success for the demo\")\n",
    "                \n",
    "                print(\"We update the input parameters here\")\n",
    "                input_files = update_input_parameters(input_files)\n",
    "            else:\n",
    "                print(f'Trial unsuccessful for input {r[\"i\"]} may want to retry')\n",
    "                print(\"You can retry failed trials here, change input files and rerun\")                \n",
    "                #For now we break\n",
    "                print(\"For the demo we break\")                \n",
    "                iteration_no = iteration_max \n",
    "        \n",
    "        iteration_no = iteration_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fa196-796d-499f-87a3-0bb0d106dabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c4b36-5055-43f8-a657-0f3b7b0025dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
